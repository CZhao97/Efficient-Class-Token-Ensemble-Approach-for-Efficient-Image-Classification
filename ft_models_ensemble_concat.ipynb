{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on one GPU.\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 313/313 [00:15<00:00, 19.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 1.1650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 313/313 [00:15<00:00, 19.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Training Loss: 0.6421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 313/313 [00:15<00:00, 19.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Training Loss: 0.5345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 313/313 [00:15<00:00, 19.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Training Loss: 0.4800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 313/313 [00:15<00:00, 19.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Training Loss: 0.4448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 313/313 [00:15<00:00, 19.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Training Loss: 0.4170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 313/313 [00:16<00:00, 19.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Training Loss: 0.4003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 313/313 [00:15<00:00, 19.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Training Loss: 0.3848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 313/313 [00:16<00:00, 19.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Training Loss: 0.3718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 313/313 [00:16<00:00, 19.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Training Loss: 0.3595\n",
      "Time taken:  159.55125284194946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test time taken:  0.03563038910491557\n",
      "Epoch 10, Validation Loss: 0.3839\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from accelerate import Accelerator\n",
    "from torchvision.models import resnet50\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, RandomHorizontalFlip, v2, Resize\n",
    "from tqdm import tqdm\n",
    "from accelerate import notebook_launcher\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import timm\n",
    "import time\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "\n",
    "class lcnet_ft(nn.Module):\n",
    "    def __init__(self, num_classes=10, embed_dim=1280):\n",
    "        super(lcnet_ft, self).__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Load and freeze lcnet\n",
    "        self.encoder = timm.create_model('timm/lcnet_075.ra2_in1k', pretrained=True, num_classes=num_classes)\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # unfreeze the last layer\n",
    "        for param in self.encoder.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "    def forward(self, x):\n",
    "        logits = self.encoder(x)  # Assuming [batch_size, embed_dim]\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "class mnasnet_ft(nn.Module):\n",
    "    def __init__(self, num_classes=10, embed_dim=1280):\n",
    "        super(mnasnet_ft, self).__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Load and freeze lcnet\n",
    "        self.encoder = timm.create_model('timm/mnasnet_small.lamb_in1k', pretrained=True, num_classes=num_classes)\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # unfreeze the last layer\n",
    "        for param in self.encoder.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "    def forward(self, x):\n",
    "        logits = self.encoder(x)  # Assuming [batch_size, embed_dim]\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "class repghostnet_ft(nn.Module):\n",
    "    def __init__(self, num_classes=10, embed_dim=1280):\n",
    "        super(repghostnet_ft, self).__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Load and freeze lcnet\n",
    "        self.encoder = timm.create_model('timm/repghostnet_050.in1k', pretrained=True, num_classes=num_classes)\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # unfreeze the last layer\n",
    "        for param in self.encoder.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "    def forward(self, x):\n",
    "        logits = self.encoder(x)  # Assuming [batch_size, embed_dim]\n",
    "        \n",
    "        return logits\n",
    "\n",
    "class ResNetViTEnsembleWithTransformer(nn.Module):\n",
    "    def __init__(self, num_classes=10, embed_dim=1280):\n",
    "        super(ResNetViTEnsembleWithTransformer, self).__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Load and freeze \n",
    "        self.lcnet = lcnet_ft(num_classes=num_classes, embed_dim=embed_dim)\n",
    "        self.mnasnet = mnasnet_ft(num_classes=num_classes, embed_dim=embed_dim)\n",
    "        self.repghostnet = repghostnet_ft(num_classes=num_classes, embed_dim=embed_dim)\n",
    "\n",
    "        self.lcnet.encoder.classifier = nn.Identity()\n",
    "        self.mnasnet.encoder.classifier = nn.Identity()\n",
    "        self.repghostnet.encoder.classifier = nn.Identity()\n",
    "\n",
    "        self.classifier = nn.Linear(embed_dim*3, num_classes)\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Get the features from the backbone\n",
    "        lcnet_features = self.lcnet.encoder(x)\n",
    "        mnasnet_features = self.mnasnet.encoder(x)\n",
    "        repghostnet_features = self.repghostnet.encoder(x)\n",
    "\n",
    "        # Final classification\n",
    "        feature_concat = torch.cat((lcnet_features, mnasnet_features, repghostnet_features), dim=1)\n",
    "        \n",
    "        # Final classification\n",
    "\n",
    "        logits = self.classifier(feature_concat)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Assuming CIFAR10 - adapt transforms for your dataset\n",
    "transform = Compose([\n",
    "    Resize((224,224)),\n",
    "    RandomHorizontalFlip(),\n",
    "    # v2.ColorJitter(brightness=(0.5,1.5),contrast=(1),saturation=(0.5,1.5),hue=(-0.1,0.1)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.247, 0.243, 0.261]),\n",
    "])\n",
    "\n",
    "# Dataset and DataLoader setup\n",
    "class FTDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "def train_model(train_loader, test_loader):\n",
    "    accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "    model = ResNetViTEnsembleWithTransformer()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    model, optimizer, train_loader, test_loader = accelerator.prepare(model, optimizer, train_loader, test_loader)\n",
    "\n",
    "    # time start\n",
    "    start = time.time()\n",
    "    for epoch in range(10):  # Adjust the number of epochs if needed\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for x, labels in tqdm(train_loader, desc=f\"Epoch {epoch + 1}\"):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1}, Training Loss: {avg_loss:.4f}\")\n",
    "    # time end\n",
    "    end = time.time()\n",
    "    print(\"Time taken: \", end-start)\n",
    "\n",
    "    # Optional test phase at the end of training\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        start = time.time()\n",
    "        for x, labels in test_loader:\n",
    "            outputs = model(x)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "        end = time.time()\n",
    "        print(\"Test time taken: \", (end-start)/len(test_loader))\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    print(f\"Epoch {epoch + 1}, Validation Loss: {avg_test_loss:.4f}\")\n",
    "\n",
    "    # Save the final model\n",
    "    model_path = \"/home/chen/EECE570/new_models/ensemble_model/model_final_concat.pt\"\n",
    "    unencapsulated_model = accelerator.unwrap_model(model)\n",
    "    accelerator.save(unencapsulated_model.state_dict(), model_path)\n",
    "\n",
    "\n",
    "def main():\n",
    "    root_dir = './data'\n",
    "    batch_size = 128\n",
    "    transform = Compose([\n",
    "        Resize((224,224)),\n",
    "        RandomHorizontalFlip(),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.247, 0.243, 0.261]),\n",
    "    ])\n",
    "    \n",
    "    # Load full dataset\n",
    "    full_dataset = CIFAR10(root=root_dir, train=True, download=True, transform=None)\n",
    "    images = [image for image, _ in full_dataset]\n",
    "    labels = [label for _, label in full_dataset]\n",
    "\n",
    "    # Stratified train-test split\n",
    "    train_images, test_images, train_labels, test_labels = train_test_split(\n",
    "        images, labels, test_size=0.2, stratify=labels, random_state=42\n",
    "    )\n",
    "\n",
    "    # Creating datasets\n",
    "    train_dataset = FTDataset(train_images, train_labels, transform=transform)\n",
    "    test_dataset = FTDataset(test_images, test_labels, transform=transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "\n",
    "    # Call train_model or any other training function here\n",
    "    train_model(train_loader, test_loader)\n",
    "\n",
    "notebook_launcher(main, num_processes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:03<00:00, 40.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8683\n",
      "Recall: 0.9986\n",
      "Precision: 0.9994\n",
      "F1 Score: 0.9990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# testing on testset\n",
    "model = ResNetViTEnsembleWithTransformer()\n",
    "model.load_state_dict(torch.load(\"/home/chen/EECE570/new_models/ensemble_model/model_final_concat.pt\"))\n",
    "model.cuda()\n",
    "\n",
    "full_dataset = CIFAR10(root='./data', train=False, download=True, transform=None)\n",
    "images = [image for image, _ in full_dataset]\n",
    "labels = [label for _, label in full_dataset]\n",
    "\n",
    "\n",
    "testset = FTDataset(images, labels, transform=transform)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=True, num_workers=8, pin_memory=True)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "total = 0\n",
    "correct = 0\n",
    "false = 0\n",
    "false_positive = 0\n",
    "false_negative = 0\n",
    "with torch.no_grad():\n",
    "    for x, labels in tqdm(testloader):\n",
    "        x, labels = x.cuda(), labels.cuda()\n",
    "        outputs = model(x)\n",
    "        predictions = torch.argmax(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        false_positive += ((predictions == 1) & (labels == 0)).sum().item()\n",
    "        false_negative += ((predictions == 0) & (labels == 1)).sum().item()\n",
    "\n",
    "\n",
    "# output accuracy, recell, precision, f1 score\n",
    "accuracy = correct / total\n",
    "recall = correct / (correct + false_negative)\n",
    "precision = correct / (correct + false_positive)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"Accuracy: {:.4f}\".format(accuracy))\n",
    "print(\"Recall: {:.4f}\".format(recall))\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"F1 Score: {:.4f}\".format(f1_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
